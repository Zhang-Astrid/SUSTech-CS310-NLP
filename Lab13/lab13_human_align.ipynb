{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b29f68",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 13: Human Alignment\n",
    "\n",
    "In this lab, we will practice two tasks:\n",
    "- Using the code framework for training a reward model that assigns scores to pairs of sentences. \n",
    "- Getting familiar with the code framework for Direct Preference Optimization (DPO).\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:40:03.271439Z",
     "start_time": "2025-05-20T08:40:00.224349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import LlamaForCausalLM"
   ],
   "id": "149117e4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lijl/anaconda3/envs/nnUnet/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data1/lijl/anaconda3/envs/nnUnet/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/data1/lijl/anaconda3/envs/nnUnet/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b2f2573d",
   "metadata": {},
   "source": [
    "## T1. Defining Reward Model\n",
    "\n",
    "\n",
    "We will use the [LlamaForCausalLM](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaForCausalLM) model from HuggingFace, as the basis for our reward model.\n",
    "\n",
    "First, two internal forward functions are to be implemented:\n",
    "- `_forward_rm`: it takes the input ids and attention masks of a sequence (user input + response), and returns the reward scores.\n",
    "  - The reward scores are in tensor of same shape as the input ids, with **one reward score for each token**.\n",
    "  - Reward scores are calculated by calling a linear layer `self.reward_head` on the last hidden state (of the entire sequence).\n",
    "- `_forward_lmloss`: it takes the input of same format, but returns the regular language modeling loss.\n",
    "  - Logits are computed by calling `self.lm_head` on the last hidden state.\n",
    "  - The `response_ids` are used as the target for the `nn.CrossEntropyLoss()`.\n",
    "\n",
    "Then, define the `forward` function, which takes the input ids and attention masks of two sequences, and returns the combined loss.\n",
    "- Compute `reward1` on the first sequence (positve example) and `reward2` on the second sequence (negative example).\n",
    "- Calculate their difference in `logits`\n",
    "- Reward loss is computed by calling `F.binary_cross_entropy_with_logits(logits, label)`."
   ]
  },
  {
   "cell_type": "code",
   "id": "14d6d247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:40:03.283397Z",
     "start_time": "2025-05-20T08:40:03.274191Z"
    }
   },
   "source": [
    "class LlamaRewardModel(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # A linear layer to map hidden states to a scalar, as the final reward\n",
    "        self.reward_head = nn.Linear(config.hidden_size, 1, bias=False)\n",
    "\n",
    "    def _forward_rm(self, input_ids, attention_mask, **kargs):\n",
    "        \"\"\"\n",
    "        input_ids: input token ids\n",
    "        attention_mask: attention mask\n",
    "        Return: reward scores, output from self.reward_head\n",
    "        \"\"\"\n",
    "        # Call self.model.forward()  to get the hidden states\n",
    "        output = self.model.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, \n",
    "            return_dict=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "        ### START YOUR CODE ###\n",
    "        # Feed the last hidden state from output to self.reward_head to get the reward score\n",
    "        rewards = self.reward_head(output.last_hidden_state)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return rewards \n",
    "    \n",
    "    def _forward_lmloss(self, prompt_ids, lm_attn_mask, response_ids):\n",
    "        \"\"\"\n",
    "        input_ids: input token ids\n",
    "        attention_mask: attention mask\n",
    "        Return: cross-entropy loss for language modeling\n",
    "        \"\"\" \n",
    "        # Call self.model.forward()  to get the hidden states\n",
    "        outputs = self.model.forward(\n",
    "            input_ids=prompt_ids,\n",
    "            attention_mask=lm_attn_mask,\n",
    "            return_dict=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        # Call self.lm_head on last hidden state to get the logits \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        # Call self.lm_head on last hidden state to get the logits\n",
    "        logits = self.lm_head(outputs.last_hidden_state)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), response_ids.view(-1))\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def forward(self, sent1_idx, attention_mask_1, sent2_idx, attention_mask_2, labels, prompt_ids, lm_attn_mask, response_ids, **kargs):\n",
    "        \"\"\"\n",
    "        sent1_idx: User input ids + positive output ids\n",
    "        attention_mask_1: Attention mask for sent1_idx\n",
    "        sent2_idx: User input ids + negative output ids\n",
    "        attention_mask_2: Attention mask for sent2_idx\n",
    "\n",
    "        labels: Positive output ids (all zeros)\n",
    "\n",
    "        prompt_ids: User input ids + positive output ids\n",
    "        lm_attn_mask: Attention mask for prompt_ids\n",
    "        response_ids: Target ids for calculating cross-entropy loss\n",
    "        \"\"\"\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        # Reward for positive example\n",
    "        reward0 = self._forward_rm(sent1_idx, attention_mask_1)\n",
    "        # Reward for negative example\n",
    "        reward1 = self._forward_rm(sent2_idx, attention_mask_2)\n",
    "        # Calculate the reward difference\n",
    "        logits = (reward0 - reward1).squeeze(-1)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        # Compute the reward modeling loss\n",
    "        rm_loss = F.binary_cross_entropy_with_logits(logits, labels.to(logits.dtype), reduction=\"mean\")\n",
    "\n",
    "        # Compute the language modeling loss \n",
    "        lm_loss = self._forward_lmloss(prompt_ids, lm_attn_mask, response_ids)\n",
    "\n",
    "        # Final loss\n",
    "        loss = rm_loss + lm_loss\n",
    "\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ffbd17a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:40:43.053738Z",
     "start_time": "2025-05-20T08:40:03.284999Z"
    }
   },
   "source": [
    "# Test\n",
    "# model = LlamaRewardModel.from_pretrained('/Users/xy/models/llama-2-7b-hf')\n",
    "# model = LlamaRewardModel.from_pretrained(\"qwen/Qwen2.5-1.5B-Instruct\")\n",
    "model = LlamaRewardModel.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# You expect to see the model correctly initialized"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lijl/anaconda3/envs/nnUnet/lib/python3.9/site-packages/huggingface_hub-0.29.2-py3.8.egg/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Some weights of LlamaRewardModel were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['reward_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "40f020bf",
   "metadata": {},
   "source": [
    "## T2. Load Preference Data\n",
    "\n",
    "We will load the preference dataset from `Anthropic/hh-rlhf` for testing."
   ]
  },
  {
   "cell_type": "code",
   "id": "bfb3638b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:40:43.991281Z",
     "start_time": "2025-05-20T08:40:43.056323Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer\n",
    "from transformers.hf_argparser import HfArg"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "242a70e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:40:44.005834Z",
     "start_time": "2025-05-20T08:40:43.993932Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class Arguments(TrainingArguments):\n",
    "    model_name_or_path: str = HfArg(\n",
    "        default='bert-base-uncased', # The path to your model\n",
    "        help=\"The model name or path\"\n",
    "    )\n",
    "    \n",
    "    # Preference dataset\n",
    "    data_path: str = HfArg(\n",
    "        default='./hh-rlhf', # The path to the preference dataset\n",
    "        help=\"The path of preference dataset, e.g., `Anthropic/hh-rlhf`\",\n",
    "    )\n",
    "\n",
    "    model_max_length: int = HfArg(default=512, help=\"Maximum sequence length.\")\n",
    "\n",
    "    bf16: bool = HfArg(\n",
    "        default=True,\n",
    "        help=\"Whether to use bf16 (mixed) precision instead of 32-bit.\",\n",
    "    )\n",
    "\n",
    "    # Hyper-parameters for DPO loss\n",
    "    beta: float = HfArg(\n",
    "        default=0.1,\n",
    "        help=\"The beta factor in DPO loss.\"\n",
    "        \"Higher beta means less divergence from the initial policy.\",\n",
    "    )\n",
    "\n",
    "    output_dir: str = HfArg(\n",
    "        default=\"output\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "ad62a0f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:40:45.888108Z",
     "start_time": "2025-05-20T08:40:44.007952Z"
    }
   },
   "source": [
    "# Test\n",
    "args = Arguments()\n",
    "print(args.model_name_or_path)\n",
    "print(args.data_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "./hh-rlhf\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "f7e7db53",
   "metadata": {},
   "source": [
    "The following function prepares the preference dataset in a user-friendly view."
   ]
  },
  {
   "cell_type": "code",
   "id": "6ace96a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:40:45.895261Z",
     "start_time": "2025-05-20T08:40:45.889712Z"
    }
   },
   "source": [
    "def get_data(split, data_path):\n",
    "    dataset = load_dataset(split=split, path=data_path)\n",
    "\n",
    "    def split_prompt_and_responses_hh(sample):\n",
    "        search_term = \"\\n\\nAssistant:\"\n",
    "        search_term_idx = sample[\"chosen\"].rfind(search_term)\n",
    "        assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "        prompt = sample[\"chosen\"][:search_term_idx + len(search_term)]\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"][len(prompt):],\n",
    "            \"rejected\": sample[\"rejected\"][len(prompt):],\n",
    "        }\n",
    "\n",
    "    return dataset.map(split_prompt_and_responses_hh)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1c5c85bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:40:59.653568Z",
     "start_time": "2025-05-20T08:40:45.896876Z"
    }
   },
   "source": [
    "# Test\n",
    "train_dataset = get_data('train', args.data_path)\n",
    "\n",
    "prompt = train_dataset[0]['prompt']\n",
    "chosen = train_dataset[0]['chosen']\n",
    "rejected = train_dataset[0]['rejected']\n",
    "print(prompt[:45])\n",
    "print(chosen)\n",
    "print(rejected)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 160800 examples [00:02, 66090.45 examples/s]\n",
      "Generating test split: 8552 examples [00:00, 57735.98 examples/s]\n",
      "Map: 100%|██████████| 160800/160800 [00:11<00:00, 14601.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      " I haven't even thought about it.\n",
      " Ass.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "7493a453",
   "metadata": {},
   "source": [
    "Now, load tokenizer and tokenize some sample data.\n",
    "\n",
    "- `sent1_encoded` is the tokenized result of `prompt + chosen` (positive example)\n",
    "- `sent2_encoded` is the tokenized result of `prompt + rejected` (negative example)"
   ]
  },
  {
   "cell_type": "code",
   "id": "61188b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:41:09.794355Z",
     "start_time": "2025-05-20T08:40:59.656216Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=False)\n",
    "\n",
    "\n",
    "### START YOUR CODE ###\n",
    "sent1_encoded = tokenizer(prompt + chosen, \n",
    "                          return_tensors=\"pt\", \n",
    "                          truncation=True, \n",
    "                          max_length=args.model_max_length)\n",
    "sent2_encoded = tokenizer(prompt + rejected, \n",
    "                          return_tensors=\"pt\", \n",
    "                          truncation=True, \n",
    "                          max_length=args.model_max_length)\n",
    "### END YOUR CODE ###"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lijl/anaconda3/envs/nnUnet/lib/python3.9/site-packages/huggingface_hub-0.29.2-py3.8.egg/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "acad4b95",
   "metadata": {},
   "source": [
    "Pad two sequences (input ids and attention masks) to same length"
   ]
  },
  {
   "cell_type": "code",
   "id": "875322d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:41:09.805274Z",
     "start_time": "2025-05-20T08:41:09.797171Z"
    }
   },
   "source": [
    "sent1_idx = sent1_encoded['input_ids']\n",
    "sent2_idx = sent2_encoded['input_ids']\n",
    "\n",
    "# Pad input ids\n",
    "max_len = max(sent1_idx.shape[1], sent2_idx.shape[1])\n",
    "sent1_idx = torch.nn.functional.pad(sent1_idx, (0, max_len - sent1_idx.shape[1]), value=tokenizer.pad_token_id)\n",
    "sent2_idx = torch.nn.functional.pad(sent2_idx, (0, max_len - sent2_idx.shape[1]), value=tokenizer.pad_token_id)\n",
    "\n",
    "# Pad attention masks\n",
    "sent1_attn_mask = sent1_encoded['attention_mask']\n",
    "sent2_attn_mask = sent2_encoded['attention_mask']\n",
    "sent1_attn_mask = torch.nn.functional.pad(sent1_attn_mask, (0, max_len - sent1_attn_mask.shape[1]), value=0)\n",
    "sent2_attn_mask = torch.nn.functional.pad(sent2_attn_mask, (0, max_len - sent2_attn_mask.shape[1]), value=0)\n",
    "\n",
    "print(sent1_idx.shape)\n",
    "print(sent2_idx.shape)\n",
    "print(sent1_attn_mask.shape)\n",
    "print(sent2_attn_mask.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 194])\n",
      "torch.Size([1, 194])\n",
      "torch.Size([1, 194])\n",
      "torch.Size([1, 194])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "e8b47f16",
   "metadata": {},
   "source": [
    "Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "id": "87d90e52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:41:09.811455Z",
     "start_time": "2025-05-20T08:41:09.807156Z"
    }
   },
   "source": [
    "input_data = {\n",
    "    'sent1_idx': sent1_idx, \n",
    "    'attention_mask_1': sent1_attn_mask, \n",
    "    'sent2_idx': sent2_idx, \n",
    "    'attention_mask_2': sent2_attn_mask, \n",
    "\n",
    "    'labels': torch.zeros_like(sent1_idx), \n",
    "\n",
    "    'prompt_ids': sent1_encoded['input_ids'], \n",
    "    'lm_attn_mask': sent1_encoded['attention_mask'], \n",
    "    'response_ids': sent1_encoded['input_ids'],\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "790ff1e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:41:32.915526Z",
     "start_time": "2025-05-20T08:41:09.813082Z"
    }
   },
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**input_data)\n",
    "    print(output)\n",
    "\n",
    "# You expect to see a single loss value\n",
    "# Runtime Error is likely to because by the implementation of the internal forward functions\n",
    "# You can use the following code to help you debug\n",
    "# r1 = model._forward_rmloss(sent1_idx, sent1_attn_mask)\n",
    "# print(r1.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.3092)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "e21fb68c",
   "metadata": {},
   "source": [
    "## T3. (Optional) DPO Training\n",
    "\n",
    "You need to install the [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/en/index) library first.\n",
    "\n",
    "```bash\n",
    "pip install trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "c534fc66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:41:33.517128Z",
     "start_time": "2025-05-20T08:41:32.918286Z"
    }
   },
   "source": [
    "from trl import DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, HfArgumentParser"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtrl\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DPOTrainer\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM, HfArgumentParser\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'trl'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "eed20643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T08:41:33.520373Z",
     "start_time": "2025-05-20T08:41:33.519999Z"
    }
   },
   "source": [
    "def train():\n",
    "    # Parse arguments\n",
    "    parser = HfArgumentParser(Arguments)\n",
    "    args = parser.parse_args_into_dataclasses()[0]\n",
    "    \n",
    "    # Load policy model\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    # Load reference model\n",
    "    model_ref = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    # Freeze reference model\n",
    "    model_ref.eval()\n",
    "    for param in model_ref.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Tokenizer and data\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        model_max_length=args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True,\n",
    "    )\n",
    "    train_dataset = get_data(\"train\", args.data_path)\n",
    "\n",
    "    # Training arguments\n",
    "    kwargs = dict(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    dpo_trainer = DPOTrainer(**kwargs)\n",
    "    dpo_trainer.train()\n",
    "    dpo_trainer.save_state()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21e2a1cc",
   "metadata": {},
   "source": [
    "train()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnU_kernel",
   "language": "python",
   "name": "nnunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

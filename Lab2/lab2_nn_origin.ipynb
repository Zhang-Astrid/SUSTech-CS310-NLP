{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 2: Neural Text Classification\n",
    "y\n",
    "\n",
    "pip install torch==2.2.0\n",
    "pip install torchtext==0.17.0\n",
    "python = 3.9\n",
    "\n",
    "\n",
    "In this lab, we will practice building a neural network for text classification. \n",
    "\n",
    "The tutorial code is adopted from the official PyTorch tutorial: *Text classification with the torchtext library*\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#text-classification-with-the-torchtext-library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install torchtext\n",
    "\n",
    "Url: https://pypi.org/project/torchtext/\n",
    "```bash\n",
    "conda install -c pytorch torchtext\n",
    "```\n",
    "\n",
    "You may or may not need to manually install the following packages:\n",
    "    \n",
    "```bash\n",
    "pip install chardet\n",
    "pip install -U portalocker>=2.0.0\n",
    "```\n",
    "\n",
    "or with conda\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge 'portalocker>=2.0.0'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:17:56.376922Z",
     "start_time": "2025-02-27T13:17:56.356126Z"
    }
   },
   "source": [
    "import torch\n",
    "from torchtext.datasets import SST2 # SST2 is the sentiment analysis dataset, binary"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T13:18:50.956433Z",
     "start_time": "2025-02-27T13:18:49.515701Z"
    }
   },
   "source": [
    "# Check the raw data\n",
    "train_iter = iter(SST2(split='train'))\n",
    "\n",
    "count = 0\n",
    "for item in train_iter:\n",
    "    print(item)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'portalocker' is not defined\nThis exception is thrown by __iter__ of _MemoryCellIterDataPipe(remember_elements=1000, source_datapipe=_ChildDataPipe)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m train_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(SST2(split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      4\u001B[0m count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m train_iter:\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(item)\n\u001B[0;32m      7\u001B[0m     count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\sharding.py:72\u001B[0m, in \u001B[0;36mShardingFilterIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe):\n\u001B[0;32m     73\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_of_instances \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstance_id:\n\u001B[0;32m     74\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m item\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combinatorics.py:124\u001B[0m, in \u001B[0;36mShufflerIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[T_co]:\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enabled:\n\u001B[1;32m--> 124\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe:\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m x\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:122\u001B[0m, in \u001B[0;36mMapperIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[T_co]:\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe:\n\u001B[0;32m    123\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_fn(data)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\plain_text_reader.py:168\u001B[0m, in \u001B[0;36m_CSVBaseParserIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Union[D, Tuple[\u001B[38;5;28mstr\u001B[39m, D]]]:\n\u001B[1;32m--> 168\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m path, file \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe:\n\u001B[0;32m    169\u001B[0m         stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_helper\u001B[38;5;241m.\u001B[39mskip_lines(file)\n\u001B[0;32m    170\u001B[0m         stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_helper\u001B[38;5;241m.\u001B[39mdecode(stream)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\fileopener.py:67\u001B[0m, in \u001B[0;36mFileOpenerIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 67\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m get_file_binaries_from_pathnames(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py:210\u001B[0m, in \u001B[0;36mget_file_binaries_from_pathnames\u001B[1;34m(pathnames, mode, encoding)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    208\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m mode\n\u001B[1;32m--> 210\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pathname \u001B[38;5;129;01min\u001B[39;00m pathnames:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pathname, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    212\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected string type for pathname, but got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    213\u001B[0m                         \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(pathname)))\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:52\u001B[0m, in \u001B[0;36mConcaterIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator:\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m dp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipes:\n\u001B[1;32m---> 52\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dp:\n\u001B[0;32m     53\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:52\u001B[0m, in \u001B[0;36mConcaterIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator:\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m dp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipes:\n\u001B[1;32m---> 52\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dp:\n\u001B[0;32m     53\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\cacheholder.py:454\u001B[0m, in \u001B[0;36m_FulfilledPromisesIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    449\u001B[0m         \u001B[38;5;66;03m# TODO(VitalyFedyunin): If no match found, that means we exceeded length of memory_cell\u001B[39;00m\n\u001B[0;32m    450\u001B[0m         \u001B[38;5;66;03m# and there is aggressive amount 1-to-zero cases, raise error and explain how to fix\u001B[39;00m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 454\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe:\n\u001B[0;32m    455\u001B[0m         rec_uuid, record \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory_cell_dp\u001B[38;5;241m.\u001B[39mget_last()\n\u001B[0;32m    456\u001B[0m         original_file_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfirst_filepath_fn(record)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\saver.py:53\u001B[0m, in \u001B[0;36mSaverIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m filepath, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe:\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     55\u001B[0m             filepath \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(filepath)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:122\u001B[0m, in \u001B[0;36mMapperIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[T_co]:\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe:\n\u001B[0;32m    123\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_fn(data)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:122\u001B[0m, in \u001B[0;36mMapperIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[T_co]:\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe:\n\u001B[0;32m    123\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_fn(data)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\selecting.py:71\u001B[0m, in \u001B[0;36mFilterIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[T_co]:\n\u001B[1;32m---> 71\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe:\n\u001B[0;32m     72\u001B[0m         condition, filtered \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_returnIfTrue(data)\n\u001B[0;32m     73\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m condition:\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\ziparchiveloader.py:53\u001B[0m, in \u001B[0;36mZipArchiveLoaderIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Tuple[\u001B[38;5;28mstr\u001B[39m, BufferedIOBase]]:\n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe:\n\u001B[0;32m     54\u001B[0m         validate_pathname_binary_tuple(data)\n\u001B[0;32m     55\u001B[0m         pathname, data_stream \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\fileopener.py:67\u001B[0m, in \u001B[0;36mFileOpenerIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 67\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m get_file_binaries_from_pathnames(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py:210\u001B[0m, in \u001B[0;36mget_file_binaries_from_pathnames\u001B[1;34m(pathnames, mode, encoding)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    208\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m mode\n\u001B[1;32m--> 210\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pathname \u001B[38;5;129;01min\u001B[39;00m pathnames:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pathname, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    212\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected string type for pathname, but got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    213\u001B[0m                         \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(pathname)))\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\cacheholder.py:229\u001B[0m, in \u001B[0;36mOnDiskCacheHolderIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    228\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_end_caching_flag:\n\u001B[1;32m--> 229\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    231\u001B[0m         \u001B[38;5;66;03m# In case of BC breaking, use RuntimeError for now. Warning is another option\u001B[39;00m\n\u001B[0;32m    232\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease call `end_caching()` before iteration.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\cacheholder.py:374\u001B[0m, in \u001B[0;36m_MemoryCellIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 374\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe:\n\u001B[0;32m    375\u001B[0m         item_id \u001B[38;5;241m=\u001B[39m uuid\u001B[38;5;241m.\u001B[39muuid4()\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer_pos \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer_pos \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremember_elements\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:144\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.IteratorDecorator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_next()\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Decided against using `contextlib.nullcontext` for performance reasons\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_next\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:132\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.IteratorDecorator._get_next\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;124;03mReturn next with logic related to iterator validity, profiler, and incrementation of samples yielded.\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m _check_iterator_valid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_dp, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator_id)\n\u001B[1;32m--> 132\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_and_has_next_method:\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_dp\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:427\u001B[0m, in \u001B[0;36m_DemultiplexerIterDataPipe.get_next_element_by_instance\u001B[1;34m(self, instance_id)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    426\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 427\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_find_next\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    428\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    429\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_child_stop[instance_id] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:396\u001B[0m, in \u001B[0;36m_DemultiplexerIterDataPipe._find_next\u001B[1;34m(self, instance_id)\u001B[0m\n\u001B[0;32m    392\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_datapipe_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    394\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_datapipe_iterator has not been set, likely because this private method is called directly \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    395\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwithout invoking get_next_element_by_instance() first.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 396\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_datapipe_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    397\u001B[0m classification \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier_fn(value)\n\u001B[0;32m    398\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m classification \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_none:\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:52\u001B[0m, in \u001B[0;36mConcaterIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator:\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m dp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipes:\n\u001B[1;32m---> 52\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dp:\n\u001B[0;32m     53\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:52\u001B[0m, in \u001B[0;36mConcaterIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator:\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m dp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipes:\n\u001B[1;32m---> 52\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dp:\n\u001B[0;32m     53\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\cacheholder.py:454\u001B[0m, in \u001B[0;36m_FulfilledPromisesIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    449\u001B[0m         \u001B[38;5;66;03m# TODO(VitalyFedyunin): If no match found, that means we exceeded length of memory_cell\u001B[39;00m\n\u001B[0;32m    450\u001B[0m         \u001B[38;5;66;03m# and there is aggressive amount 1-to-zero cases, raise error and explain how to fix\u001B[39;00m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 454\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe:\n\u001B[0;32m    455\u001B[0m         rec_uuid, record \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory_cell_dp\u001B[38;5;241m.\u001B[39mget_last()\n\u001B[0;32m    456\u001B[0m         original_file_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfirst_filepath_fn(record)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\saver.py:53\u001B[0m, in \u001B[0;36mSaverIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m filepath, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe:\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     55\u001B[0m             filepath \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(filepath)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\hashchecker.py:67\u001B[0m, in \u001B[0;36mHashCheckerIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Tuple[\u001B[38;5;28mstr\u001B[39m, StreamWrapper]]:\n\u001B[1;32m---> 67\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m file_name, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe:\n\u001B[0;32m     68\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhash_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msha256\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     69\u001B[0m             hash_func \u001B[38;5;241m=\u001B[39m hashlib\u001B[38;5;241m.\u001B[39msha256()\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:122\u001B[0m, in \u001B[0;36mMapperIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[T_co]:\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe:\n\u001B[0;32m    123\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_fn(data)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:122\u001B[0m, in \u001B[0;36mMapperIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[T_co]:\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatapipe:\n\u001B[0;32m    123\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_fn(data)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\load\\online.py:84\u001B[0m, in \u001B[0;36mHTTPReaderIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Tuple[\u001B[38;5;28mstr\u001B[39m, StreamWrapper]]:\n\u001B[1;32m---> 84\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m url \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe:\n\u001B[0;32m     85\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     86\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m _get_response_from_http(url, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery_params)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\cacheholder.py:229\u001B[0m, in \u001B[0;36mOnDiskCacheHolderIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    228\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_end_caching_flag:\n\u001B[1;32m--> 229\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    231\u001B[0m         \u001B[38;5;66;03m# In case of BC breaking, use RuntimeError for now. Warning is another option\u001B[39;00m\n\u001B[0;32m    232\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease call `end_caching()` before iteration.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:173\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.wrap_generator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m         response \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39msend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    176\u001B[0m     datapipe\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\cacheholder.py:374\u001B[0m, in \u001B[0;36m_MemoryCellIterDataPipe.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 374\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_datapipe:\n\u001B[0;32m    375\u001B[0m         item_id \u001B[38;5;241m=\u001B[39m uuid\u001B[38;5;241m.\u001B[39muuid4()\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer_pos \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer_pos \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremember_elements\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:144\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.IteratorDecorator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_next()\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Decided against using `contextlib.nullcontext` for performance reasons\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_next\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_hook_iterator.py:132\u001B[0m, in \u001B[0;36mhook_iterator.<locals>.IteratorDecorator._get_next\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;124;03mReturn next with logic related to iterator validity, profiler, and incrementation of samples yielded.\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m _check_iterator_valid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_dp, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator_id)\n\u001B[1;32m--> 132\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_and_has_next_method:\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_dp\u001B[38;5;241m.\u001B[39m_number_of_samples_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:427\u001B[0m, in \u001B[0;36m_DemultiplexerIterDataPipe.get_next_element_by_instance\u001B[1;34m(self, instance_id)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    426\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 427\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_find_next\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    428\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    429\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_child_stop[instance_id] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:397\u001B[0m, in \u001B[0;36m_DemultiplexerIterDataPipe._find_next\u001B[1;34m(self, instance_id)\u001B[0m\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    394\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_datapipe_iterator has not been set, likely because this private method is called directly \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    395\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwithout invoking get_next_element_by_instance() first.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    396\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_datapipe_iterator)\n\u001B[1;32m--> 397\u001B[0m classification \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassifier_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m classification \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_none:\n\u001B[0;32m    399\u001B[0m     StreamWrapper\u001B[38;5;241m.\u001B[39mclose_streams(value)\n",
      "File \u001B[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\cacheholder.py:262\u001B[0m, in \u001B[0;36mOnDiskCacheHolderIterDataPipe._cache_check_fn\u001B[1;34m(data, filepath_fn, hash_dict, hash_type, extra_check_fn, cache_uuid)\u001B[0m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(dirname):\n\u001B[0;32m    260\u001B[0m     os\u001B[38;5;241m.\u001B[39mmakedirs(dirname)\n\u001B[1;32m--> 262\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mportalocker\u001B[49m\u001B[38;5;241m.\u001B[39mLock(promise_filepath, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma+\u001B[39m\u001B[38;5;124m\"\u001B[39m, flags\u001B[38;5;241m=\u001B[39mportalocker\u001B[38;5;241m.\u001B[39mLockFlags\u001B[38;5;241m.\u001B[39mEXCLUSIVE) \u001B[38;5;28;01mas\u001B[39;00m promise_fh:\n\u001B[0;32m    263\u001B[0m     promise_fh\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    264\u001B[0m     data \u001B[38;5;241m=\u001B[39m promise_fh\u001B[38;5;241m.\u001B[39mread()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'portalocker' is not defined\nThis exception is thrown by __iter__ of _MemoryCellIterDataPipe(remember_elements=1000, source_datapipe=_ChildDataPipe)"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the output of yield_tokens()\n",
    "count = 0\n",
    "for tokens in yield_tokens(iter(SST2(split='train'))): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 3:\n",
    "        break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary\n",
    "\n",
    "First, we build the vocabulary using the `build_vocab_from_iterator` function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(iter(SST2(split='train'))), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all strings are converted into integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the vocab\n",
    "print(vocab(['here', 'is', 'an', 'example']))\n",
    "print(vocab(['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']))\n",
    "print(vocab(['of', 'saucy']))\n",
    "\n",
    "# What about unknown words, i.e., out-of-vocabulary (OOV) words?\n",
    "print(vocab(['here', 'is', 'a', '@#$@!#$%']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, further define the `text_pipeline` and `label_pipeline` functions, for converting strings to integers. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:31:49.606788Z",
     "start_time": "2025-02-25T11:31:49.549790Z"
    }
   },
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "# Test text_pipeline()\n",
    "tokens = text_pipeline('here is the an example')\n",
    "print(tokens)\n",
    "\n",
    "# Test label_pipeline()\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m label_pipeline \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mint\u001B[39m(x)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Test text_pipeline()\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m tokens \u001B[38;5;241m=\u001B[39m \u001B[43mtext_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhere is the an example\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokens)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Test label_pipeline()\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[19], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m text_pipeline \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mvocab\u001B[49m(tokenizer(x))\n\u001B[0;32m      2\u001B[0m label_pipeline \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mint\u001B[39m(x)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Test text_pipeline()\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Batchify Data \n",
    "\n",
    "Your goal is to define the `Collate_batch` function, which will be used to process the \"raw\" data batch.\n",
    "\n",
    "*Hint*: In the loop of `collate_batch`, the labels, tokens, and the offsets of all the examples are collected into three lists. Finally, the lists are converted into tensors. \n",
    "\n",
    "*Hint*: The `offsets` need to contain the cumulative positions of tokens in the batch. \n",
    "For example, if the batch contains 3 examples, whose lengths are `[1,3,2]`, then the final offsets should be `[0,1,4,6]`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        token_ids = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0)) # Note that offsets contains the length (number of tokens) of each example\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    labels = None\n",
    "    offsets = None\n",
    "    token_ids = None\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the defined `collate_batch` function to create the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use collate_batch to generate the dataloader\n",
    "train_iter = SST2(split=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:31:49.684880Z",
     "start_time": "2025-02-25T11:31:49.645859Z"
    }
   },
   "source": [
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens in this batch: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 0: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 7: ', token_ids[offsets[7]:])\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# Number of tokens in this batch:  82\n",
    "# Number of examples in one batch:  8\n",
    "# Example 0:  tensor([ 4579,    92, 13266,    38,     1,  7742, 10000])\n",
    "# Example 7:  tensor([   5, 7100])"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Test the dataloader\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (labels, token_ids, offsets) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[43mdataloader\u001B[49m):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Define the Model\n",
    "\n",
    "The model consists of two parts of parameters: an embedding layer and a fully connected layer.\n",
    "\n",
    "Your need to first initialize an `nn.EmbeddingBag` instance and a `nn.Linear` instance:\n",
    "- The embedding layer should be initialized with `vocab_size`, `embed_dim`, and `sparse=False`.\n",
    "- The fully connected layer should have `embed_dim` as input size and `num_class` as output size.\n",
    "\n",
    "Then, in the `forward` function, the embedding layer should called with `token_ids` and `offsets` as inputs. The output of embedding layer is fed to the fully connected layer to get the final output. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        self.embedding = None\n",
    "        self.fc = None\n",
    "        ### END YOUR CODE ###\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        ### START YOUR CODE ###\n",
    "        embedded = None\n",
    "        out = None\n",
    "        ### END YOUR CODE ###\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "train_iter = iter(SST2(split='train'))\n",
    "num_class = len(set([label for (_, label) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64 # embedding size\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "print('output size:', output.size())\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# output size: torch.Size([8, 2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Define the loss function\n",
    "\n",
    "Cross entropy loss should be used. You can use `torch.nn.CrossEntropyLoss` to define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:31:49.732443Z",
     "start_time": "2025-02-25T11:31:49.696860Z"
    }
   },
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "### START YOUR CODE ###\n",
    "criterion = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m criterion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m### END YOUR CODE ###\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(\u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLR)\n\u001B[0;32m     14\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mStepLR(optimizer, \u001B[38;5;241m1.0\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the loss function `criterion` works\n",
    "\n",
    "In the cell below, implement the **manual computation** of cross entropy loss. Use the formula $-y_i\\log(\\hat{y_i})$, \n",
    "\n",
    "where $y_i$ is the $i$-th ground truth label in `labels`, and $\\hat{y_i}$ is the predicted probability in `output` of the correponding label."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:31:49.735446Z",
     "start_time": "2025-02-25T11:31:49.735446Z"
    }
   },
   "source": [
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "print('output shape:', output.shape)\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "# Manually calculate the loss\n",
    "loss_manual = []\n",
    "for i in range(output.shape[0]):\n",
    "    ### START YOUR CODE ###\n",
    "    probs = None\n",
    "    l = None\n",
    "    ### END YOUR CODE ###\n",
    "    loss_manual.append(l)\n",
    "loss_manual = torch.stack(loss_manual)\n",
    "print('loss_manual mean:', loss_manual.mean())\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# output shape: torch.Size([8, 2])\n",
    "# loss: tensor(XXXXX)\n",
    "# loss_manual mean: tensor(XXXXX)\n",
    "# loss equals loss_manual: tensor(True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Train and Evaluate Functions\n",
    "\n",
    "Define train and evaluate functions.\n",
    "\n",
    "You need to implement the forward pass, loss computation, backward propagation, and parameter update in the `train` function. \n",
    "\n",
    "Also, for each batch of data, calculate the total number of correctly predicted examples, by comparing `output` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        ### START YOUR CODE ###\n",
    "        # Forward pass\n",
    "        output = None\n",
    "        ### END YOUR CODE ###\n",
    "        try:\n",
    "            ### START YOUR CODE ###\n",
    "            # Compute loss\n",
    "            loss = None\n",
    "            ### END YOUR CODE ###\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            print('token_ids: ', token_ids)\n",
    "            print('offsets: ', offsets)\n",
    "            raise\n",
    "        ### START YOUR CODE ###\n",
    "        # Backward propagation, grad clipping, and optimization\n",
    "        pass # call loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        pass # call optimizer.step()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        # Calculate correct prediction in current batch\n",
    "        total_acc += None\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        ### START YOUR CODE ###\n",
    "        # Similar to the code in train function, but without backpropagation\n",
    "        output = None\n",
    "        loss = None\n",
    "        total_acc += None\n",
    "        ### END YOUR CODE ###\n",
    "        total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train, valid, and test data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:31:49.984009Z",
     "start_time": "2025-02-25T11:31:49.786967Z"
    }
   },
   "source": [
    "# Prepare train, valid, and test data\n",
    "train_iter = SST2(split=\"train\")\n",
    "test_iter = SST2(split=\"test\")\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DILL_AVAILABLE' from 'torch.utils.data.datapipes.utils.common' (C:\\Users\\ZS\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Prepare train, valid, and test data\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m train_iter \u001B[38;5;241m=\u001B[39m \u001B[43mSST2\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m test_iter \u001B[38;5;241m=\u001B[39m SST2(split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m to_map_style_dataset(train_iter)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\basic\\lib\\site-packages\\torchtext\\data\\datasets_utils.py:193\u001B[0m, in \u001B[0;36m_create_dataset_directory.<locals>.decorator.<locals>.wrapper\u001B[1;34m(root, *args, **kwargs)\u001B[0m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(new_root):\n\u001B[0;32m    192\u001B[0m     os\u001B[38;5;241m.\u001B[39mmakedirs(new_root, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(root\u001B[38;5;241m=\u001B[39mnew_root, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\basic\\lib\\site-packages\\torchtext\\data\\datasets_utils.py:155\u001B[0m, in \u001B[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001B[1;34m(root, split, **kwargs)\u001B[0m\n\u001B[0;32m    153\u001B[0m result \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m _check_default_set(split, splits, fn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m):\n\u001B[1;32m--> 155\u001B[0m     result\u001B[38;5;241m.\u001B[39mappend(fn(root, item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _wrap_datasets(\u001B[38;5;28mtuple\u001B[39m(result), split)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\basic\\lib\\site-packages\\torchtext\\datasets\\sst2.py:87\u001B[0m, in \u001B[0;36mSST2\u001B[1;34m(root, split)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_module_available(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorchdata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m(\n\u001B[0;32m     85\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPackage `torchdata` not found. Please install following instructions at https://github.com/pytorch/data\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     86\u001B[0m     )\n\u001B[1;32m---> 87\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatapipes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FileOpener, GDriveReader, HttpReader, IterableWrapper  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m     89\u001B[0m url_dp \u001B[38;5;241m=\u001B[39m IterableWrapper([URL])\n\u001B[0;32m     90\u001B[0m cache_compressed_dp \u001B[38;5;241m=\u001B[39m url_dp\u001B[38;5;241m.\u001B[39mon_disk_cache(\n\u001B[0;32m     91\u001B[0m     filepath_fn\u001B[38;5;241m=\u001B[39mpartial(_filepath_fn, root),\n\u001B[0;32m     92\u001B[0m     hash_dict\u001B[38;5;241m=\u001B[39m{_filepath_fn(root): MD5},\n\u001B[0;32m     93\u001B[0m     hash_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmd5\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     94\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\basic\\lib\\site-packages\\torchdata\\__init__.py:9\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# All rights reserved.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _extension  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datapipes\n\u001B[0;32m     11\u001B[0m janitor \u001B[38;5;241m=\u001B[39m datapipes\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mjanitor\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\basic\\lib\\site-packages\\torchdata\\datapipes\\__init__.py:9\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# All rights reserved.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataChunk, functional_datapipe\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;28miter\u001B[39m, \u001B[38;5;28mmap\u001B[39m, utils\n\u001B[0;32m     11\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataChunk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctional_datapipe\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miter\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmap\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutils\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\basic\\lib\\site-packages\\torchdata\\datapipes\\iter\\__init__.py:79\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatapipes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransform\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     69\u001B[0m     BatchAsyncMapperIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m BatchAsyncMapper,\n\u001B[0;32m     70\u001B[0m     BatchMapperIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m BatchMapper,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     76\u001B[0m     ThreadPoolMapperIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m ThreadPoolMapper,\n\u001B[0;32m     77\u001B[0m )\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatapipes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbz2fileloader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Bz2FileLoaderIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m Bz2FileLoader\n\u001B[1;32m---> 79\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatapipes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcacheholder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     80\u001B[0m     EndOnDiskCacheHolderIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m EndOnDiskCacheHolder,\n\u001B[0;32m     81\u001B[0m     InMemoryCacheHolderIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m InMemoryCacheHolder,\n\u001B[0;32m     82\u001B[0m     OnDiskCacheHolderIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m OnDiskCacheHolder,\n\u001B[0;32m     83\u001B[0m )\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatapipes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcombining\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     85\u001B[0m     IterKeyZipperIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m IterKeyZipper,\n\u001B[0;32m     86\u001B[0m     MapKeyZipperIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m MapKeyZipper,\n\u001B[0;32m     87\u001B[0m     RoundRobinDemultiplexerIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m RoundRobinDemultiplexer,\n\u001B[0;32m     88\u001B[0m     UnZipperIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m UnZipper,\n\u001B[0;32m     89\u001B[0m )\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatapipes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcycler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CyclerIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m Cycler, RepeaterIterDataPipe \u001B[38;5;28;01mas\u001B[39;00m Repeater\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\basic\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\cacheholder.py:24\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m     22\u001B[0m     portalocker \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatapipes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _check_unpickable_fn, DILL_AVAILABLE\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgraph\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m traverse_dps\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatapipes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional_datapipe\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'DILL_AVAILABLE' from 'torch.utils.data.datapipes.utils.common' (C:\\Users\\ZS\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py)"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"text_classification_model.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Test Data\n",
    "\n",
    "This is a necessary step. But since the `test` split of SST2 is not annotated, we will use the `dev` split here to pretend it is the test data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))\n",
    "\n",
    "# Your test accuracy should be around 0.9"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Test the model with a few unannotated examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sentiment_labels = ['negative', 'positive']\n",
    "\n",
    "def predict(text, model, vocab, tokenizer, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(vocab(tokenizer(text)), device=device)\n",
    "        output = model(text, torch.tensor([0], device=device))\n",
    "        return labels[output.argmax(1).item()]\n",
    "\n",
    "ex_text_str = \"A very well-made, funny and entertaining picture.\"\n",
    "print(\"This is a %s sentiment.\" % (predict(ex_text_str, model, vocab, tokenizer, sentiment_labels)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully built and trained a neural network model to classify sentiment in text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 2: Neural Text Classification\n",
    "\n",
    "In this lab, we will practice building a neural network for text classification. \n",
    "\n",
    "The tutorial code is adopted from the official PyTorch tutorial: *Text classification with the torchtext library*\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#text-classification-with-the-torchtext-library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install datasets\n",
    "\n",
    "Url: https://huggingface.co/docs/datasets/en/installation\n",
    "```bash\n",
    "conda install -c huggingface -c conda-forge datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:49:26.542145Z",
     "start_time": "2025-03-03T18:49:26.530665Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "from data_utils import DatasetIterator, get_tokenizer, build_vocab_from_iter, to_map_style_dataset"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:49:58.368731Z",
     "start_time": "2025-03-03T18:49:26.811404Z"
    }
   },
   "source": [
    "# Test datasets package, with SST2\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('glue', 'sst2') # it take a while"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:49:58.398855Z",
     "start_time": "2025-03-03T18:49:58.371635Z"
    }
   },
   "source": [
    "# Check the raw data\n",
    "train_iter = DatasetIterator(ds['train'])\n",
    "\n",
    "count = 0\n",
    "for item in train_iter:\n",
    "    print(item)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hide new secretions from the parental units ', 0)\n",
      "('contains no wit , only labored gags ', 0)\n",
      "('that loves its characters and communicates something rather beautiful about human nature ', 1)\n",
      "('remains utterly satisfied to remain the same throughout ', 0)\n",
      "('on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', 0)\n",
      "(\"that 's far too tragic to merit such superficial treatment \", 0)\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:49:58.430851Z",
     "start_time": "2025-03-03T18:49:58.402855Z"
    }
   },
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)"
   ],
   "outputs": [],
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:49:58.460879Z",
     "start_time": "2025-03-03T18:49:58.437855Z"
    }
   },
   "source": [
    "# Check the output of yield_tokens()\n",
    "count = 0\n",
    "for tokens in yield_tokens(DatasetIterator(ds['train'])): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 3:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']\n",
      "['contains', 'no', 'wit', ',', 'only', 'labored', 'gags']\n",
      "['that', 'loves', 'its', 'characters', 'and', 'communicates', 'something', 'rather', 'beautiful', 'about', 'human', 'nature']\n",
      "['remains', 'utterly', 'satisfied', 'to', 'remain', 'the', 'same', 'throughout']\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary\n",
    "\n",
    "First, we build the vocabulary using the `build_vocab_from_iterator` function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:04.674275Z",
     "start_time": "2025-03-03T18:49:58.465869Z"
    }
   },
   "source": [
    "vocab = build_vocab_from_iter(yield_tokens(DatasetIterator(ds['train'])), specials=[\"<unk>\"])"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all strings are converted into integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:04.689767Z",
     "start_time": "2025-03-03T18:50:04.676775Z"
    }
   },
   "source": [
    "# Check the vocab\n",
    "print(vocab(['here', 'is', 'an', 'example']))\n",
    "print(vocab(['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']))\n",
    "print(vocab(['of', 'saucy']))\n",
    "\n",
    "# What about unknown words, i.e., out-of-vocabulary (OOV) words?\n",
    "print(vocab(['here', 'is', 'a', '@#$@!#$%']))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 10, 16, 1587]\n",
      "[4460, 92, 12405, 38, 1, 7259, 8990]\n",
      "[5, 6549]\n",
      "[224, 10, 3, 0]\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, further define the `text_pipeline` and `label_pipeline` functions, for converting strings to integers. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:04.705379Z",
     "start_time": "2025-03-03T18:50:04.692378Z"
    }
   },
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "# Test text_pipeline()\n",
    "tokens = text_pipeline('here is an example')\n",
    "print(tokens)\n",
    "\n",
    "# Test label_pipeline()\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 10, 16, 1587]\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Batchify Data \n",
    "\n",
    "Your goal is to define the `Collate_batch` function, which will be used to process the \"raw\" data batch.\n",
    "\n",
    "*Hint*: In the loop of `collate_batch`, the labels, tokens, and the offsets of all the examples are collected into three lists. Finally, the lists are converted into tensors. \n",
    "\n",
    "*Hint*: The `offsets` need to contain the cumulative positions of tokens in the batch. \n",
    "For example, if the batch contains 3 examples, whose lengths are `[1,3,2]`, then the final offsets should be `[0,1,4,6]`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:04.721377Z",
     "start_time": "2025-03-03T18:50:04.707376Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        token_ids = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0)) # Note that offsets contains the length (number of tokens) of each example\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    # 将标签列表转换为张量\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # 将所有token_ids拼接成一个大的tensor\n",
    "    token_ids = torch.cat(token_ids_list, dim=0)\n",
    "    # 计算偏移量的累积值\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)  # 用cumsum计算累积偏移量\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the defined `collate_batch` function to create the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:04.737377Z",
     "start_time": "2025-03-03T18:50:04.724378Z"
    }
   },
   "source": [
    "# Use collate_batch to generate the dataloader\n",
    "train_iter = DatasetIterator(ds['train'])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:04.753384Z",
     "start_time": "2025-03-03T18:50:04.742377Z"
    }
   },
   "source": [
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens in this batch: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 0: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 7: ', token_ids[offsets[7]:])\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# Number of tokens in this batch:  82\n",
    "# Number of examples in one batch:  8\n",
    "# Example 0:  tensor([ 4460,    92, 12405,    38,     1,  7259,  8990])\n",
    "# Example 7:  tensor([   5, 6549])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in this batch:  82\n",
      "Number of examples in one batch:  8\n",
      "Example 0:  tensor([ 4460,    92, 12405,    38,     1,  7259,  8990])\n",
      "Example 7:  tensor([   5, 6549])\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Define the Model\n",
    "\n",
    "The model consists of two parts of parameters: an embedding layer and a fully connected layer.\n",
    "\n",
    "Your need to first initialize an `nn.EmbeddingBag` instance and a `nn.Linear` instance:\n",
    "- The embedding layer should be initialized with `vocab_size`, `embed_dim`, and `sparse=False`.\n",
    "- The fully connected layer should have `embed_dim` as input size and `num_class` as output size.\n",
    "\n",
    "Then, in the `forward` function, the embedding layer should called with `token_ids` and `offsets` as inputs. The output of embedding layer is fed to the fully connected layer to get the final output. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:04.769385Z",
     "start_time": "2025-03-03T18:50:04.756376Z"
    }
   },
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)  # 初始化EmbeddingBag层\n",
    "        self.fc = nn.Linear(embed_dim, num_class)  # 初始化全连接层\n",
    "        ### END YOUR CODE ###\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        ### START YOUR CODE ###\n",
    "        embedded = self.embedding(token_ids, offsets)  # 使用embedding层\n",
    "        out = self.fc(embedded)  # 将嵌入输出传给全连接层\n",
    "        ### END YOUR CODE ###\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:09.489062Z",
     "start_time": "2025-03-03T18:50:04.772392Z"
    }
   },
   "source": [
    "# Build the model\n",
    "train_iter = DatasetIterator(ds['train'])\n",
    "num_class = len(set([label for (_, label) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64 # embedding size\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:09.504233Z",
     "start_time": "2025-03-03T18:50:09.491068Z"
    }
   },
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "print('output size:', output.size())\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# output size: torch.Size([8, 2])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Define the loss function\n",
    "\n",
    "Cross entropy loss should be used. You can use `torch.nn.CrossEntropyLoss` to define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:09.520260Z",
     "start_time": "2025-03-03T18:50:09.507259Z"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the loss function `criterion` works\n",
    "\n",
    "In the cell below, implement the **manual computation** of cross entropy loss. Use the formula $-y_i\\log(\\hat{y_i})$, \n",
    "\n",
    "where $y_i$ is the $i$-th ground truth label in `labels`, and $\\hat{y_i}$ is the predicted probability in `output` of the correponding label."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:09.552260Z",
     "start_time": "2025-03-03T18:50:09.522260Z"
    }
   },
   "source": [
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "print('output shape:', output.shape)\n",
    "\n",
    "\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "# Manually calculate the loss\n",
    "loss_manual = []\n",
    "for i in range(output.shape[0]):\n",
    "    probs = torch.nn.functional.softmax(output[i], dim=-1)  # 使用softmax计算预测的概率\n",
    "    # 获取对应标签的概率\n",
    "    correct_prob = probs[labels[i]]\n",
    "    # 使用交叉熵公式计算损失\n",
    "    l = -torch.log(correct_prob)\n",
    "    loss_manual.append(l)\n",
    "loss_manual = torch.stack(loss_manual)\n",
    "print('loss_manual mean:', loss_manual.mean())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 output: tensor([[ 0.0618, -0.0895],\n",
      "        [-0.1845, -0.1673],\n",
      "        [ 0.1175,  0.1440],\n",
      "        [ 0.0052, -0.3349],\n",
      "        [ 0.1299, -0.3681],\n",
      "        [-0.1505, -0.0839],\n",
      "        [ 0.0480,  0.1447],\n",
      "        [-0.6846,  0.6014]])\n",
      "output shape: torch.Size([8, 2])\n",
      "loss: tensor(0.5789)\n",
      "loss_manual mean: tensor(0.5789)\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Train and Evaluate Functions\n",
    "\n",
    "Define train and evaluate functions.\n",
    "\n",
    "You need to implement the forward pass, loss computation, backward propagation, and parameter update in the `train` function. \n",
    "\n",
    "Also, for each batch of data, calculate the total number of correctly predicted examples, by comparing `output` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:09.583265Z",
     "start_time": "2025-03-03T18:50:09.555260Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        ### START YOUR CODE ###\n",
    "        # Forward pass\n",
    "        output = model(token_ids, offsets)\n",
    "        ### END YOUR CODE ###\n",
    "        try:\n",
    "            ### START YOUR CODE ###\n",
    "            # Compute loss\n",
    "            loss = criterion(output, labels)\n",
    "            ### END YOUR CODE ###\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            print('token_ids: ', token_ids)\n",
    "            print('offsets: ', offsets)\n",
    "            raise\n",
    "        ### START YOUR CODE ###\n",
    "        # Backward propagation, grad clipping, and optimization\n",
    "        loss.backward()  # 反向传播\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)  # 梯度裁剪，防止梯度爆炸\n",
    "        optimizer.step()  # 更新参数\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        # Calculate correct prediction in current batch\n",
    "        _, predicted = output.max(1)  # 获取预测的类别\n",
    "        total_acc += (predicted == labels).sum().item()  # 计算正确的预测数量并累加\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        ### START YOUR CODE ###\n",
    "        # Similar to the code in train function, but without backpropagation\n",
    "        # 前向传播，计算输出\n",
    "        output = model(text, offsets)\n",
    "        # 计算损失\n",
    "        loss = criterion(output, label)\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        # 计算正确的预测数量\n",
    "        _, predicted = output.max(1)\n",
    "        total_acc += (predicted == label).sum().item()\n",
    "        ### END YOUR CODE ###\n",
    "        total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count"
   ],
   "outputs": [],
   "execution_count": 103
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train, valid, and test data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:50:14.451244Z",
     "start_time": "2025-03-03T18:50:09.585773Z"
    }
   },
   "source": [
    "# Prepare train, valid, and test data\n",
    "train_iter = DatasetIterator(ds['train'])\n",
    "test_iter = DatasetIterator(ds['test'])\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:54:08.174761Z",
     "start_time": "2025-03-03T18:50:14.454244Z"
    }
   },
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 7998 batches | accuracy    0.592\n",
      "| epoch   1 |  1000/ 7998 batches | accuracy    0.665\n",
      "| epoch   1 |  1500/ 7998 batches | accuracy    0.721\n",
      "| epoch   1 |  2000/ 7998 batches | accuracy    0.741\n",
      "| epoch   1 |  2500/ 7998 batches | accuracy    0.759\n",
      "| epoch   1 |  3000/ 7998 batches | accuracy    0.779\n",
      "| epoch   1 |  3500/ 7998 batches | accuracy    0.800\n",
      "| epoch   1 |  4000/ 7998 batches | accuracy    0.805\n",
      "| epoch   1 |  4500/ 7998 batches | accuracy    0.812\n",
      "| epoch   1 |  5000/ 7998 batches | accuracy    0.809\n",
      "| epoch   1 |  5500/ 7998 batches | accuracy    0.822\n",
      "| epoch   1 |  6000/ 7998 batches | accuracy    0.828\n",
      "| epoch   1 |  6500/ 7998 batches | accuracy    0.830\n",
      "| epoch   1 |  7000/ 7998 batches | accuracy    0.838\n",
      "| epoch   1 |  7500/ 7998 batches | accuracy    0.837\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 24.07s | valid accuracy    0.856 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 7998 batches | accuracy    0.867\n",
      "| epoch   2 |  1000/ 7998 batches | accuracy    0.873\n",
      "| epoch   2 |  1500/ 7998 batches | accuracy    0.878\n",
      "| epoch   2 |  2000/ 7998 batches | accuracy    0.881\n",
      "| epoch   2 |  2500/ 7998 batches | accuracy    0.885\n",
      "| epoch   2 |  3000/ 7998 batches | accuracy    0.872\n",
      "| epoch   2 |  3500/ 7998 batches | accuracy    0.882\n",
      "| epoch   2 |  4000/ 7998 batches | accuracy    0.877\n",
      "| epoch   2 |  4500/ 7998 batches | accuracy    0.882\n",
      "| epoch   2 |  5000/ 7998 batches | accuracy    0.886\n",
      "| epoch   2 |  5500/ 7998 batches | accuracy    0.882\n",
      "| epoch   2 |  6000/ 7998 batches | accuracy    0.897\n",
      "| epoch   2 |  6500/ 7998 batches | accuracy    0.885\n",
      "| epoch   2 |  7000/ 7998 batches | accuracy    0.883\n",
      "| epoch   2 |  7500/ 7998 batches | accuracy    0.898\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 23.36s | valid accuracy    0.878 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 7998 batches | accuracy    0.905\n",
      "| epoch   3 |  1000/ 7998 batches | accuracy    0.905\n",
      "| epoch   3 |  1500/ 7998 batches | accuracy    0.906\n",
      "| epoch   3 |  2000/ 7998 batches | accuracy    0.903\n",
      "| epoch   3 |  2500/ 7998 batches | accuracy    0.893\n",
      "| epoch   3 |  3000/ 7998 batches | accuracy    0.899\n",
      "| epoch   3 |  3500/ 7998 batches | accuracy    0.904\n",
      "| epoch   3 |  4000/ 7998 batches | accuracy    0.906\n",
      "| epoch   3 |  4500/ 7998 batches | accuracy    0.904\n",
      "| epoch   3 |  5000/ 7998 batches | accuracy    0.903\n",
      "| epoch   3 |  5500/ 7998 batches | accuracy    0.909\n",
      "| epoch   3 |  6000/ 7998 batches | accuracy    0.909\n",
      "| epoch   3 |  6500/ 7998 batches | accuracy    0.904\n",
      "| epoch   3 |  7000/ 7998 batches | accuracy    0.897\n",
      "| epoch   3 |  7500/ 7998 batches | accuracy    0.907\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 23.31s | valid accuracy    0.883 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 7998 batches | accuracy    0.913\n",
      "| epoch   4 |  1000/ 7998 batches | accuracy    0.915\n",
      "| epoch   4 |  1500/ 7998 batches | accuracy    0.914\n",
      "| epoch   4 |  2000/ 7998 batches | accuracy    0.915\n",
      "| epoch   4 |  2500/ 7998 batches | accuracy    0.917\n",
      "| epoch   4 |  3000/ 7998 batches | accuracy    0.921\n",
      "| epoch   4 |  3500/ 7998 batches | accuracy    0.917\n",
      "| epoch   4 |  4000/ 7998 batches | accuracy    0.907\n",
      "| epoch   4 |  4500/ 7998 batches | accuracy    0.913\n",
      "| epoch   4 |  5000/ 7998 batches | accuracy    0.913\n",
      "| epoch   4 |  5500/ 7998 batches | accuracy    0.911\n",
      "| epoch   4 |  6000/ 7998 batches | accuracy    0.914\n",
      "| epoch   4 |  6500/ 7998 batches | accuracy    0.911\n",
      "| epoch   4 |  7000/ 7998 batches | accuracy    0.911\n",
      "| epoch   4 |  7500/ 7998 batches | accuracy    0.917\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 23.54s | valid accuracy    0.898 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 7998 batches | accuracy    0.930\n",
      "| epoch   5 |  1000/ 7998 batches | accuracy    0.918\n",
      "| epoch   5 |  1500/ 7998 batches | accuracy    0.920\n",
      "| epoch   5 |  2000/ 7998 batches | accuracy    0.926\n",
      "| epoch   5 |  2500/ 7998 batches | accuracy    0.929\n",
      "| epoch   5 |  3000/ 7998 batches | accuracy    0.914\n",
      "| epoch   5 |  3500/ 7998 batches | accuracy    0.924\n",
      "| epoch   5 |  4000/ 7998 batches | accuracy    0.920\n",
      "| epoch   5 |  4500/ 7998 batches | accuracy    0.921\n",
      "| epoch   5 |  5000/ 7998 batches | accuracy    0.921\n",
      "| epoch   5 |  5500/ 7998 batches | accuracy    0.922\n",
      "| epoch   5 |  6000/ 7998 batches | accuracy    0.917\n",
      "| epoch   5 |  6500/ 7998 batches | accuracy    0.914\n",
      "| epoch   5 |  7000/ 7998 batches | accuracy    0.920\n",
      "| epoch   5 |  7500/ 7998 batches | accuracy    0.922\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 23.45s | valid accuracy    0.903 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 7998 batches | accuracy    0.929\n",
      "| epoch   6 |  1000/ 7998 batches | accuracy    0.931\n",
      "| epoch   6 |  1500/ 7998 batches | accuracy    0.929\n",
      "| epoch   6 |  2000/ 7998 batches | accuracy    0.924\n",
      "| epoch   6 |  2500/ 7998 batches | accuracy    0.926\n",
      "| epoch   6 |  3000/ 7998 batches | accuracy    0.928\n",
      "| epoch   6 |  3500/ 7998 batches | accuracy    0.939\n",
      "| epoch   6 |  4000/ 7998 batches | accuracy    0.923\n",
      "| epoch   6 |  4500/ 7998 batches | accuracy    0.922\n",
      "| epoch   6 |  5000/ 7998 batches | accuracy    0.931\n",
      "| epoch   6 |  5500/ 7998 batches | accuracy    0.918\n",
      "| epoch   6 |  6000/ 7998 batches | accuracy    0.927\n",
      "| epoch   6 |  6500/ 7998 batches | accuracy    0.924\n",
      "| epoch   6 |  7000/ 7998 batches | accuracy    0.920\n",
      "| epoch   6 |  7500/ 7998 batches | accuracy    0.927\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 23.32s | valid accuracy    0.898 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 7998 batches | accuracy    0.944\n",
      "| epoch   7 |  1000/ 7998 batches | accuracy    0.945\n",
      "| epoch   7 |  1500/ 7998 batches | accuracy    0.942\n",
      "| epoch   7 |  2000/ 7998 batches | accuracy    0.945\n",
      "| epoch   7 |  2500/ 7998 batches | accuracy    0.944\n",
      "| epoch   7 |  3000/ 7998 batches | accuracy    0.940\n",
      "| epoch   7 |  3500/ 7998 batches | accuracy    0.950\n",
      "| epoch   7 |  4000/ 7998 batches | accuracy    0.947\n",
      "| epoch   7 |  4500/ 7998 batches | accuracy    0.943\n",
      "| epoch   7 |  5000/ 7998 batches | accuracy    0.948\n",
      "| epoch   7 |  5500/ 7998 batches | accuracy    0.942\n",
      "| epoch   7 |  6000/ 7998 batches | accuracy    0.953\n",
      "| epoch   7 |  6500/ 7998 batches | accuracy    0.948\n",
      "| epoch   7 |  7000/ 7998 batches | accuracy    0.951\n",
      "| epoch   7 |  7500/ 7998 batches | accuracy    0.947\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 23.21s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 7998 batches | accuracy    0.950\n",
      "| epoch   8 |  1000/ 7998 batches | accuracy    0.950\n",
      "| epoch   8 |  1500/ 7998 batches | accuracy    0.953\n",
      "| epoch   8 |  2000/ 7998 batches | accuracy    0.949\n",
      "| epoch   8 |  2500/ 7998 batches | accuracy    0.947\n",
      "| epoch   8 |  3000/ 7998 batches | accuracy    0.952\n",
      "| epoch   8 |  3500/ 7998 batches | accuracy    0.952\n",
      "| epoch   8 |  4000/ 7998 batches | accuracy    0.954\n",
      "| epoch   8 |  4500/ 7998 batches | accuracy    0.949\n",
      "| epoch   8 |  5000/ 7998 batches | accuracy    0.944\n",
      "| epoch   8 |  5500/ 7998 batches | accuracy    0.949\n",
      "| epoch   8 |  6000/ 7998 batches | accuracy    0.950\n",
      "| epoch   8 |  6500/ 7998 batches | accuracy    0.953\n",
      "| epoch   8 |  7000/ 7998 batches | accuracy    0.949\n",
      "| epoch   8 |  7500/ 7998 batches | accuracy    0.949\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 23.20s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  1000/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  1500/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  2000/ 7998 batches | accuracy    0.954\n",
      "| epoch   9 |  2500/ 7998 batches | accuracy    0.949\n",
      "| epoch   9 |  3000/ 7998 batches | accuracy    0.957\n",
      "| epoch   9 |  3500/ 7998 batches | accuracy    0.956\n",
      "| epoch   9 |  4000/ 7998 batches | accuracy    0.952\n",
      "| epoch   9 |  4500/ 7998 batches | accuracy    0.952\n",
      "| epoch   9 |  5000/ 7998 batches | accuracy    0.955\n",
      "| epoch   9 |  5500/ 7998 batches | accuracy    0.957\n",
      "| epoch   9 |  6000/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  6500/ 7998 batches | accuracy    0.957\n",
      "| epoch   9 |  7000/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  7500/ 7998 batches | accuracy    0.954\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 23.08s | valid accuracy    0.917 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 7998 batches | accuracy    0.958\n",
      "| epoch  10 |  1000/ 7998 batches | accuracy    0.954\n",
      "| epoch  10 |  1500/ 7998 batches | accuracy    0.953\n",
      "| epoch  10 |  2000/ 7998 batches | accuracy    0.946\n",
      "| epoch  10 |  2500/ 7998 batches | accuracy    0.952\n",
      "| epoch  10 |  3000/ 7998 batches | accuracy    0.953\n",
      "| epoch  10 |  3500/ 7998 batches | accuracy    0.958\n",
      "| epoch  10 |  4000/ 7998 batches | accuracy    0.953\n",
      "| epoch  10 |  4500/ 7998 batches | accuracy    0.955\n",
      "| epoch  10 |  5000/ 7998 batches | accuracy    0.957\n",
      "| epoch  10 |  5500/ 7998 batches | accuracy    0.955\n",
      "| epoch  10 |  6000/ 7998 batches | accuracy    0.950\n",
      "| epoch  10 |  6500/ 7998 batches | accuracy    0.955\n",
      "| epoch  10 |  7000/ 7998 batches | accuracy    0.947\n",
      "| epoch  10 |  7500/ 7998 batches | accuracy    0.954\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 23.17s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:54:08.190441Z",
     "start_time": "2025-03-03T18:54:08.176763Z"
    }
   },
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"text_classification_model.pth\")"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Test Data\n",
    "\n",
    "This is a necessary step. But since the `test` split of SST2 is not annotated, we will use the `dev` split here to pretend it is the test data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:54:08.486529Z",
     "start_time": "2025-03-03T18:54:08.192443Z"
    }
   },
   "source": [
    "accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))\n",
    "\n",
    "# Your test accuracy should be around 0.9"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.916\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Test the model with a few unannotated examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T18:54:08.502223Z",
     "start_time": "2025-03-03T18:54:08.489527Z"
    }
   },
   "source": [
    "sentiment_labels = ['negative', 'positive']\n",
    "\n",
    "def predict(text, model, vocab, tokenizer, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(vocab(tokenizer(text)), device=device)\n",
    "        output = model(text, torch.tensor([0], device=device))\n",
    "        return labels[output.argmax(1).item()]\n",
    "\n",
    "ex_text_str = \"A very well-made, funny and entertaining picture.\"\n",
    "print(\"This is a %s sentiment.\" % (predict(ex_text_str, model, vocab, tokenizer, sentiment_labels)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a positive sentiment.\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully built and trained a neural network model to classify sentiment in text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

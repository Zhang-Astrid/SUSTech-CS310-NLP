{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 1. Neural Text Classification\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from Lab 2. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0. Import Necessary Libraries"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install datasets\n",
    "\n",
    "Url: https://huggingface.co/docs/datasets/en/installation\n",
    "```bash\n",
    "conda install -c huggingface -c conda-forge datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:19.437791Z",
     "start_time": "2025-03-06T20:56:19.425304Z"
    }
   },
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from torch.utils.data.dataset import random_split\n",
    "from data_utils import DatasetIterator, get_tokenizer, build_vocab_from_iter, to_map_style_dataset"
   ],
   "outputs": [],
   "execution_count": 469
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Load data"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:19.828100Z",
     "start_time": "2025-03-06T20:56:19.733811Z"
    }
   },
   "source": [
    "test_sentences = []\n",
    "train_sentences = []\n",
    "test_url = \"test.jsonl\"\n",
    "train_url = \"train.jsonl\"\n",
    "with open(test_url, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        test_sentences.append((data[\"sentence\"], data[\"label\"][0]))\n",
    "\n",
    "with open(train_url, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        train_sentences.append((data[\"sentence\"], data[\"label\"][0]))\n",
    "\n",
    "print(len(test_sentences))\n",
    "print(len(train_sentences))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651\n",
      "12677\n"
     ]
    }
   ],
   "execution_count": 470
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Apply Tokenizer"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:19.843743Z",
     "start_time": "2025-03-06T20:56:19.831102Z"
    }
   },
   "source": [
    "# Basic tokenizer\n",
    "def basic_tokenizer(s):\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]', s)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Improved tokenizer\n",
    "def improved_tokenizer(s):\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]|[0-9]+|[a-zA-Z]+|[^\\u4e00-\\u9fff\\da-zA-Z\\s]', s)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# yeild tokenizer\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield improved_tokenizer(text)\n",
    "\n",
    "\n",
    "# Check the output of yield_tokens()\n",
    "count = 0\n",
    "for tokens in yield_tokens(iter(train_sentences)):\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 3:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '说']\n",
      "['保', '姆', '小', '张', '说', '：', '干', '啥', '子', '嘛', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '你', '看', '你', '往', '星', '空', '看', '月', '朦', '胧', '，', '鸟', '朦', '胧']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '咱', '是', '不', '是', '歇', '一', '下', '这', '双', '，', '疲', '惫', '的', '双', '腿', '？']\n"
     ]
    }
   ],
   "execution_count": 471
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Build Vocabulary"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:19.983450Z",
     "start_time": "2025-03-06T20:56:19.846789Z"
    }
   },
   "source": [
    "train_dataset = iter(train_sentences)\n",
    "test_dataset = iter(test_sentences)\n",
    "vocab = build_vocab_from_iter(yield_tokens(train_dataset), specials=[\"<unk>\"])\n",
    "print('vocab size:', len(vocab))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2823\n"
     ]
    }
   ],
   "execution_count": 472
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:19.999113Z",
     "start_time": "2025-03-06T20:56:19.986299Z"
    }
   },
   "source": [
    "# Check the vocab\n",
    "print(vocab(['卖', '油', '条', '这']))\n",
    "print(vocab(['11', '5']))\n",
    "\n",
    "print(vocab(['！', '。', '，', '@#$@!#$%']))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[473, 457, 283, 10]\n",
      "[1738, 1547]\n",
      "[69, 302, 6, 0]\n"
     ]
    }
   ],
   "execution_count": 473
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.015653Z",
     "start_time": "2025-03-06T20:56:20.001319Z"
    }
   },
   "source": [
    "text_pipeline = lambda x: vocab(improved_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "# Test text_pipeline()\n",
    "tokens = text_pipeline('卖油条')\n",
    "print(tokens)\n",
    "\n",
    "# Test label_pipeline()\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[473, 457, 283]\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 474
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Batchify Data "
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.046866Z",
     "start_time": "2025-03-06T20:56:20.037214Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        token_ids = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))  # Note that offsets contains the length (number of tokens) of each example\n",
    "\n",
    "    # 将标签列表转换为张量\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # 将所有token_ids拼接成一个大的tensor\n",
    "    token_ids = torch.cat(token_ids_list, dim=0)\n",
    "    # 计算偏移量的累积值\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)  # 用cumsum计算累积偏移量\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)"
   ],
   "outputs": [],
   "execution_count": 475
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.092760Z",
     "start_time": "2025-03-06T20:56:20.075889Z"
    }
   },
   "source": [
    "# Use collate_batch to generate the dataloader\n",
    "train_iter = train_sentences\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 476
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.170405Z",
     "start_time": "2025-03-06T20:56:20.151568Z"
    }
   },
   "source": [
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens in this batch: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 0: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 7: ', token_ids[offsets[7]:])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in this batch:  121\n",
      "Number of examples in one batch:  8\n",
      "Example 0:  tensor([473, 457, 283,  23, 424,   1,   2,   3,   1])\n",
      "Example 7:  tensor([ 473,  457,  283,   23,  424,    1,    2, 1188,  222])\n"
     ]
    }
   ],
   "execution_count": 477
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2. Build the Model"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.232756Z",
     "start_time": "2025-03-06T20:56:20.208928Z"
    }
   },
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hidden_dim1, hidden_dim2):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)  # 初始化EmbeddingBag层\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim2, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        for layer in self.hidden_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "                layer.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        embedded = self.embedding(token_ids, offsets)  # 使用embedding层\n",
    "        out = self.hidden_layers(embedded)\n",
    "        return self.fc(out)"
   ],
   "outputs": [],
   "execution_count": 478
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.295409Z",
     "start_time": "2025-03-06T20:56:20.270384Z"
    }
   },
   "source": [
    "# Build the model\n",
    "train_iter = train_sentences\n",
    "test_iter = test_sentences\n",
    "num_class = len(set([label for (_, label) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64  # embedding size\n",
    "hidden_dim1 = 16\n",
    "hidden_dim2 = 8\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class, hidden_dim1, hidden_dim2).to(device)"
   ],
   "outputs": [],
   "execution_count": 479
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.326808Z",
     "start_time": "2025-03-06T20:56:20.307974Z"
    }
   },
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "print('output size:', output.size())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n"
     ]
    }
   ],
   "execution_count": 480
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3. Train and Evaluate"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.373014Z",
     "start_time": "2025-03-06T20:56:20.351543Z"
    }
   },
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(token_ids, offsets)\n",
    "        try:\n",
    "            # Compute loss\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            print('token_ids: ', token_ids)\n",
    "            print('offsets: ', offsets)\n",
    "            raise\n",
    "        # Backward propagation, grad clipping, and optimization\n",
    "        loss.backward()  # 反向传播\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)  # 梯度裁剪，防止梯度爆炸\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        # Calculate correct prediction in current batch\n",
    "        _, predicted = output.max(1)  # 获取预测的类别\n",
    "        total_acc += (predicted == labels).sum().item()  # 计算正确的预测数量并累加\n",
    "\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    y_labels = []\n",
    "    y_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            output = model(text, offsets)\n",
    "            predictions = output.argmax(1)\n",
    "\n",
    "            total_acc += (predictions == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "            y_labels.extend(label.tolist())\n",
    "            y_preds.extend(predictions.tolist())\n",
    "\n",
    "    accuracy = total_acc / total_count\n",
    "    precision, recall, f1Score, _ = precision_recall_fscore_support(y_labels, y_preds, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1Score"
   ],
   "outputs": [],
   "execution_count": 481
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.436605Z",
     "start_time": "2025-03-06T20:56:20.422757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ],
   "outputs": [],
   "execution_count": 482
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.483811Z",
     "start_time": "2025-03-06T20:56:20.465346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "print('output shape:', output.shape)\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "# Manually calculate the loss\n",
    "loss_manual = []\n",
    "for i in range(output.shape[0]):\n",
    "    probs = torch.nn.functional.softmax(output[i], dim=-1)  # 使用softmax计算预测的概率\n",
    "    # 获取对应标签的概率\n",
    "    correct_prob = probs[labels[i]]\n",
    "    # 使用交叉熵公式计算损失\n",
    "    l = -torch.log(correct_prob)\n",
    "    loss_manual.append(l)\n",
    "loss_manual = torch.stack(loss_manual)\n",
    "print('loss_manual mean:', loss_manual.mean())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 output: tensor([[-0.1171,  0.0866],\n",
      "        [-0.2377,  0.0669],\n",
      "        [-0.0183,  0.0112],\n",
      "        [-0.0592,  0.0150],\n",
      "        [-0.1017,  0.0010],\n",
      "        [-0.0892,  0.0736],\n",
      "        [-0.2351,  0.0953],\n",
      "        [-0.1376,  0.0119]])\n",
      "output shape: torch.Size([8, 2])\n",
      "loss: tensor(0.7588)\n",
      "loss_manual mean: tensor(0.7588)\n"
     ]
    }
   ],
   "execution_count": 483
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Load train, valid, and test data"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:20.577055Z",
     "start_time": "2025-03-06T20:56:20.549295Z"
    }
   },
   "source": [
    "# Prepare train, valid, and test data\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 484
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:50.110235Z",
     "start_time": "2025-03-06T20:56:20.629285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val, precision, recall, f1 = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1506 batches | accuracy    0.693\n",
      "| epoch   1 |  1000/ 1506 batches | accuracy    0.691\n",
      "| epoch   1 |  1500/ 1506 batches | accuracy    0.685\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  3.51s | valid accuracy    0.702 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   500/ 1506 batches | accuracy    0.695\n",
      "| epoch   2 |  1000/ 1506 batches | accuracy    0.659\n",
      "| epoch   2 |  1500/ 1506 batches | accuracy    0.704\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.84s | valid accuracy    0.702 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |   500/ 1506 batches | accuracy    0.704\n",
      "| epoch   3 |  1000/ 1506 batches | accuracy    0.691\n",
      "| epoch   3 |  1500/ 1506 batches | accuracy    0.683\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.88s | valid accuracy    0.702 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   500/ 1506 batches | accuracy    0.688\n",
      "| epoch   4 |  1000/ 1506 batches | accuracy    0.698\n",
      "| epoch   4 |  1500/ 1506 batches | accuracy    0.698\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  3.03s | valid accuracy    0.702 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |   500/ 1506 batches | accuracy    0.701\n",
      "| epoch   5 |  1000/ 1506 batches | accuracy    0.684\n",
      "| epoch   5 |  1500/ 1506 batches | accuracy    0.699\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  3.07s | valid accuracy    0.702 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |   500/ 1506 batches | accuracy    0.688\n",
      "| epoch   6 |  1000/ 1506 batches | accuracy    0.683\n",
      "| epoch   6 |  1500/ 1506 batches | accuracy    0.711\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  2.92s | valid accuracy    0.702 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |   500/ 1506 batches | accuracy    0.686\n",
      "| epoch   7 |  1000/ 1506 batches | accuracy    0.701\n",
      "| epoch   7 |  1500/ 1506 batches | accuracy    0.699\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.75s | valid accuracy    0.702 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |   500/ 1506 batches | accuracy    0.686\n",
      "| epoch   8 |  1000/ 1506 batches | accuracy    0.703\n",
      "| epoch   8 |  1500/ 1506 batches | accuracy    0.690\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  2.87s | valid accuracy    0.527 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1506 batches | accuracy    0.715\n",
      "| epoch   9 |  1000/ 1506 batches | accuracy    0.723\n",
      "| epoch   9 |  1500/ 1506 batches | accuracy    0.719\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  2.76s | valid accuracy    0.707 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1506 batches | accuracy    0.730\n",
      "| epoch  10 |  1000/ 1506 batches | accuracy    0.721\n",
      "| epoch  10 |  1500/ 1506 batches | accuracy    0.719\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.83s | valid accuracy    0.705 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 485
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:50.125673Z",
     "start_time": "2025-03-06T20:56:50.113385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"A1_out.pth\")"
   ],
   "outputs": [],
   "execution_count": 486
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Test Data\n",
    "\n",
    "This is a necessary step. But since the `test` split of SST2 is not annotated, we will use the `dev` split here to pretend it is the test data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:50.203494Z",
     "start_time": "2025-03-06T20:56:50.128341Z"
    }
   },
   "source": [
    "accu_val, precision, recall, f1_score = evaluate(model, test_dataloader, criterion)\n",
    "print(\"test accuracy {:8.3f}, precision {:8.3f}, recall {:8.3f}, f1_score {:8.3f}\".format(accu_val, precision, recall,\n",
    "                                                                                          f1_score))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.757, precision    0.751, recall    0.757, f1_score    0.684\n"
     ]
    }
   ],
   "execution_count": 487
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4. Explore Word Segmentation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:50.218910Z",
     "start_time": "2025-03-06T20:56:50.206498Z"
    }
   },
   "cell_type": "code",
   "source": "import jieba",
   "outputs": [],
   "execution_count": 488
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:50.234358Z",
     "start_time": "2025-03-06T20:56:50.220911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def jieba_tokenizer(s):\n",
    "    tokens = jieba.lcut(s)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        tokens = jieba_tokenizer(text)\n",
    "        yield tokens"
   ],
   "outputs": [],
   "execution_count": 489
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:52.093932Z",
     "start_time": "2025-03-06T20:56:50.236359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = build_vocab_from_iter(yield_tokens(train_dataset), specials=[\"<unk>\"])\n",
    "print('vocab size:', len(vocab))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 13847\n"
     ]
    }
   ],
   "execution_count": 490
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:52.109359Z",
     "start_time": "2025-03-06T20:56:52.097362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the vocab\n",
    "print(vocab(['卖', '油', '条', '这']))\n",
    "print(vocab(['11', '5']))\n",
    "\n",
    "print(vocab(['！', '。', '，', '@#$@!#$%']))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[385, 2871, 1968, 16]\n",
      "[2278, 1977]\n",
      "[43, 153, 4, 0]\n"
     ]
    }
   ],
   "execution_count": 491
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:52.124063Z",
     "start_time": "2025-03-06T20:56:52.112427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_pipeline = lambda x: vocab(improved_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "# Test text_pipeline()\n",
    "tokens = text_pipeline('卖油条')\n",
    "print(tokens)\n",
    "\n",
    "# Test label_pipeline()\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[385, 2871, 1968]\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 492
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:52.139672Z",
     "start_time": "2025-03-06T20:56:52.126611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use collate_batch to generate the dataloader\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 493
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:52.171222Z",
     "start_time": "2025-03-06T20:56:52.144016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build the model\n",
    "num_class = len(set([label for (_, label) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64  # embedding size\n",
    "hidden_dim1 = 16\n",
    "hidden_dim2 = 8\n",
    "model1 = TextClassificationModel(vocab_size, emsize, num_class, hidden_dim1, hidden_dim2).to(device)"
   ],
   "outputs": [],
   "execution_count": 494
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:52.186563Z",
     "start_time": "2025-03-06T20:56:52.173222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ],
   "outputs": [],
   "execution_count": 495
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:56:52.202162Z",
     "start_time": "2025-03-06T20:56:52.188895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare train, valid, and test data\n",
    "train_iter = iter(train_sentences)\n",
    "test_iter = iter(test_sentences)\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 496
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:57:46.184444Z",
     "start_time": "2025-03-06T20:56:52.203370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model1, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val, precision, recall, f1 = evaluate(model1, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1506 batches | accuracy    0.686\n",
      "| epoch   1 |  1000/ 1506 batches | accuracy    0.697\n",
      "| epoch   1 |  1500/ 1506 batches | accuracy    0.696\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  5.40s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   500/ 1506 batches | accuracy    0.700\n",
      "| epoch   2 |  1000/ 1506 batches | accuracy    0.686\n",
      "| epoch   2 |  1500/ 1506 batches | accuracy    0.685\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  5.76s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |   500/ 1506 batches | accuracy    0.680\n",
      "| epoch   3 |  1000/ 1506 batches | accuracy    0.687\n",
      "| epoch   3 |  1500/ 1506 batches | accuracy    0.705\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  5.95s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   500/ 1506 batches | accuracy    0.689\n",
      "| epoch   4 |  1000/ 1506 batches | accuracy    0.673\n",
      "| epoch   4 |  1500/ 1506 batches | accuracy    0.701\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  5.37s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |   500/ 1506 batches | accuracy    0.702\n",
      "| epoch   5 |  1000/ 1506 batches | accuracy    0.684\n",
      "| epoch   5 |  1500/ 1506 batches | accuracy    0.667\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  5.05s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |   500/ 1506 batches | accuracy    0.691\n",
      "| epoch   6 |  1000/ 1506 batches | accuracy    0.673\n",
      "| epoch   6 |  1500/ 1506 batches | accuracy    0.693\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  5.12s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |   500/ 1506 batches | accuracy    0.707\n",
      "| epoch   7 |  1000/ 1506 batches | accuracy    0.665\n",
      "| epoch   7 |  1500/ 1506 batches | accuracy    0.682\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  5.07s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |   500/ 1506 batches | accuracy    0.693\n",
      "| epoch   8 |  1000/ 1506 batches | accuracy    0.691\n",
      "| epoch   8 |  1500/ 1506 batches | accuracy    0.685\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  5.62s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |   500/ 1506 batches | accuracy    0.687\n",
      "| epoch   9 |  1000/ 1506 batches | accuracy    0.697\n",
      "| epoch   9 |  1500/ 1506 batches | accuracy    0.684\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  5.35s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  10 |   500/ 1506 batches | accuracy    0.692\n",
      "| epoch  10 |  1000/ 1506 batches | accuracy    0.701\n",
      "| epoch  10 |  1500/ 1506 batches | accuracy    0.687\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  5.29s | valid accuracy    0.721 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 497
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:57:46.200370Z",
     "start_time": "2025-03-06T20:57:46.185946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "torch.save(model1.state_dict(), \"A1_out_jieba.pth\")"
   ],
   "outputs": [],
   "execution_count": 498
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T20:57:46.293693Z",
     "start_time": "2025-03-06T20:57:46.202455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accu_val, precision, recall, f1_score = evaluate(model1, test_dataloader, criterion)\n",
    "print(\"test accuracy {:8.3f}, precision {:8.3f}, recall {:8.3f}, f1_score {:8.3f}\".format(accu_val, precision, recall,\n",
    "                                                                                          f1_score))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.739, precision    0.546, recall    0.739, f1_score    0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZS\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 499
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
